{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram and Bigram Induction\n",
    "11/13/16 - Implements unigram and bigram induction.\n",
    "Uses the IMDB dataset folder (http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sentiment_utils import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import pickle as pickle\n",
    "imdb_folder_location = \"../aclImdb\" # Change this to wherever your imbd folder is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Bigram and Unigram Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000"
     ]
    }
   ],
   "source": [
    "## Builds maps over the neighboring contexts. Each token is weighted based on its frequency in that context over the\n",
    "# training set and its score.\n",
    "pos_both_neighboring_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_preceding_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_following_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_both_neighboring_unigrams = defaultdict(lambda : defaultdict(int))\n",
    "for index, (filename, review, score) in enumerate(imdb_sentiment_reader(dataset_type='train', sentiment='pos')):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the preceding bigram if it exists\n",
    "        preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "        # Get the following bigram if it exists\n",
    "        following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "        # Get the preceding and following unigrams if they exist\n",
    "        preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "        following_unigram = tokens[i+1] if i < len(tokens) - 1 else None\n",
    "        \n",
    "        # Insert into the appropriate maps\n",
    "        if preceding_bigram is not None and following_bigram is not None:\n",
    "            pos_both_neighboring_bigrams[(preceding_bigram, following_bigram)][tokens[i]] += score\n",
    "        if preceding_bigram is not None:\n",
    "            pos_preceding_bigrams[preceding_bigram][tokens[i]] += score\n",
    "        if following_bigram is not None:\n",
    "            pos_following_bigrams[following_bigram][tokens[i]] += score\n",
    "        if preceding_unigram is not None and following_unigram is not None:\n",
    "            pos_both_neighboring_unigrams[(preceding_unigram, following_unigram)][tokens[i]] += score\n",
    "    if index % 1000 == 0:\n",
    "        print \"Now on: \" + str(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Builds maps over the neighboring contexts. Each token is weighted based on its frequency in that context over the\n",
    "# training set and its score.\n",
    "neg_both_neighboring_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_preceding_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_following_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_both_neighboring_unigrams = defaultdict(lambda : defaultdict(int))\n",
    "for index, (filename, review, score) in enumerate(imdb_sentiment_reader(dataset_type='train', sentiment='neg')):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the preceding bigram if it exists\n",
    "        preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "        # Get the following bigram if it exists\n",
    "        following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "        # Get the preceding and following unigrams if they exist\n",
    "        preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "        following_unigram = tokens[i+1] if i < len(tokens) - 1 else None\n",
    "        \n",
    "        # Insert into the appropriate maps\n",
    "        if preceding_bigram is not None and following_bigram is not None:\n",
    "            neg_both_neighboring_bigrams[(preceding_bigram, following_bigram)][tokens[i]] -= score\n",
    "        if preceding_bigram is not None:\n",
    "            neg_preceding_bigrams[preceding_bigram][tokens[i]] -= score\n",
    "        if following_bigram is not None:\n",
    "            neg_following_bigrams[following_bigram][tokens[i]] -= score\n",
    "        if preceding_unigram is not None and following_unigram is not None:\n",
    "            neg_both_neighboring_unigrams[(preceding_unigram, following_unigram)][tokens[i]] -= score\n",
    "    if index % 1000 == 0:\n",
    "        print \"Now on: \" + str(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the dictionaries\n",
    "both_neighboring_bigrams = {\"pos\": pos_both_neighboring_bigrams, \"neg\": neg_both_neighboring_bigrams}\n",
    "preceding_bigrams = {\"pos\": pos_preceding_bigrams, \"neg\": neg_preceding_bigrams}\n",
    "following_bigrams = {\"pos\": pos_following_bigrams, \"neg\": neg_following_bigrams}\n",
    "both_neighboring_unigrams = {\"pos\": pos_both_neighboring_unigrams, \"neg\": neg_both_neighboring_unigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def induction_transform_func(filename, review, score):\n",
    "    \"\"\"\n",
    "    Baseline: returns a review with 'not' inserted in front of any identified adjectives/adverbs.\n",
    "    \"\"\"\n",
    "    def get_best_replacement(words_to_scores, score_type):\n",
    "        \"\"\"\n",
    "        Attempts to find a replacement, but returns None if the replacement is not the correct part of speech\n",
    "        \"\"\"\n",
    "        if score_type == \"pos\":\n",
    "            return sorted(words_to_scores, key=words_to_scores.get, reverse=True)[0]\n",
    "        else:\n",
    "             return sorted(words_to_scores, key=words_to_scores.get)[0]\n",
    "    \n",
    "    score_type = \"pos\" if score < 7 else \"neg\" # We want the opposite review type\n",
    "    upper_tokens = word_tokenize(review)\n",
    "    tagged_review = nltk.pos_tag(upper_tokens)\n",
    "    transformed_review = []\n",
    "    tokens = [token.lower() for token in upper_tokens]\n",
    "    if len(tokens) != len(tagged_review):\n",
    "        # Return the original review\n",
    "        return review\n",
    "    for i, tagged_word in enumerate(tagged_review):\n",
    "        # Attempt to find a replacement\n",
    "        replacement_token = tagged_word[0]\n",
    "        if tagged_word[1] in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:\n",
    "            # Get all the preceding unigrams_bigrams\n",
    "            # Get the preceding bigram if it exists\n",
    "            preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "            # Get the following bigram if it exists\n",
    "            following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "            # Get the preceding and following unigrams if they exist\n",
    "            preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "            following_unigram = tokens[i+1] if i < len(tokens) - 1 else None    \n",
    "            # Try each in turn, checking if we have the right Part of Speech\n",
    "            if (preceding_bigram, following_bigram) in both_neighboring_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(both_neighboring_bigrams[score_type][(preceding_bigram, following_bigram)], score_type)\n",
    "            elif (preceding_unigram, following_unigram) in both_neighboring_unigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(both_neighboring_unigrams[score_type][(preceding_unigram, following_unigram)], score_type)\n",
    "            elif preceding_bigram in preceding_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(preceding_bigrams[score_type][preceding_bigram], score_type)\n",
    "            elif following_bigram in following_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(following_bigrams[score_type][following_bigram], score_type)\n",
    "        transformed_review.append(replacement_token)\n",
    "        # Only reverse if new word has same POS as old word\n",
    "    return \" \".join(transformed_review)\n",
    "# Example usage:\n",
    "for (filename, review, score) in imdb_sentiment_reader(dataset_type='val', sentiment='pos'):\n",
    "    print \"Original review: \"\n",
    "    print review\n",
    "    print \"Transformed review:\" \n",
    "    transformed = induction_transform_func(filename, review, score)\n",
    "    print transformed\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_reader = imdb_sentiment_reader(dataset_type='train', sentiment='both')\n",
    "test_reader = imdb_sentiment_reader(dataset_type='val', sentiment='both')\n",
    "default_evaluator = DefaultEvaluator(verbose=True)\n",
    "baseline_runner = ExperimentRunner(train_reader, test_reader, induction_transform_func, \n",
    "                               evaluator=default_evaluator, verbose=True)\n",
    "baseline_runner.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
