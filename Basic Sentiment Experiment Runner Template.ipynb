{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Experiment Runner Template\n",
    "10/23/16 - Basic pipeline to run sentiment experiments on the IMDB movie review corpus.\n",
    "Uses the IMDB dataset folder (http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cathy/anaconda/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "imdb_folder_location = \"../aclImdb\" # Change this to wherever your imbd folder is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500_8.txt This movie was a sicky sweet cutesy romantic comedy, just the kind of movie I usually dislike but this one was just cute enough to keep me interested. It was really funny in one moment (probably why I liked it) and then just as serious in the next. Plus, it had Ellen in it and I've always had a soft spot for her.<br /><br />Basically, the owner of a book store, Helen (Kate Capshaw) finds a love letter in one of the old couches in her store. She thinks it is for her and goes crazy trying to figure out who sent it. She has kind of shut herself off from the world, so it really throws her for a loop. Eventually, almost everyone connected with her finds this letter and they are all getting mixed signals which creates some really funny moments.<br /><br />Like I said, I am usually not one for this type of movie but I really wound up enjoying it and recommend it highly. 8\n"
     ]
    }
   ],
   "source": [
    "def imdb_sentiment_reader(dataset_type='train', sentiment='both', folder_location=imdb_folder_location):\n",
    "    \"\"\"\n",
    "    Iterator over the imdb dataset.\n",
    "    Args:\n",
    "        is_train: ['train', 'val', 'test] - whether to iterate over the train, val, or test sets.\n",
    "        sentiment: ['pos', 'neg', 'both']: whether to iterate over just the positive, just the\n",
    "                   negative, or both.\n",
    "    Returns: Iterator over (filename, movie_review, sentiment_score) tuples. \n",
    "    \"\"\"\n",
    "    subfolder = 'train' if dataset_type=='train' else 'test'\n",
    "    dataset_path = os.path.join(folder_location, subfolder)\n",
    "    if sentiment=='pos' or sentiment=='both':\n",
    "        # Sort by the index\n",
    "        filenames = sorted(os.listdir(os.path.join(dataset_path, 'pos')), \n",
    "                                key=lambda filename: int(filename.split('_')[0]))\n",
    "        # Take a slice if these are for val/test\n",
    "        if dataset_type == 'val' or dataset_type == 'test':\n",
    "            cutoff = int(math.ceil(len(filenames) * .2))\n",
    "            if dataset_type == 'val':\n",
    "                filenames = filenames[:cutoff]\n",
    "            else:\n",
    "                filenames = filenames[cutoff:]\n",
    "        for filename in filenames:\n",
    "            sentiment_score = int(filename.split('_')[1].split('.')[0])\n",
    "            with open(os.path.join(dataset_path, 'pos', filename)) as f:\n",
    "                review = f.read()\n",
    "            yield filename, review.decode('utf-8'), sentiment_score\n",
    "    if sentiment=='neg' or sentiment=='both':\n",
    "        # Sort by the index \n",
    "        filenames = sorted(os.listdir(os.path.join(dataset_path, 'neg')), \n",
    "                                key=lambda filename: int(filename.split('_')[0]))\n",
    "         # Take a slice if these are for val/test\n",
    "        if dataset_type == 'val' or dataset_type == 'test':\n",
    "            cutoff = int(math.ceil(len(filenames) * .2))\n",
    "            if dataset_type == 'val':\n",
    "                filenames = filenames[:cutoff]\n",
    "            else:\n",
    "                filenames = filenames[cutoff:]\n",
    "        for filename in filenames:\n",
    "            sentiment_score = int(filename.split('_')[1].split('.')[0])\n",
    "            with open(os.path.join(dataset_path, 'neg', filename)) as f:\n",
    "                review = f.read()\n",
    "            yield filename, review.decode('utf-8'), sentiment_score\n",
    "    \n",
    "# ExampleUsage\n",
    "for filename, review, score in imdb_sentiment_reader(dataset_type='test', sentiment='both'):\n",
    "    print filename, review, score\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nevaluator = DefaultEvaluator(verbose=True)\\nfor (filename, review, score) in imdb_sentiment_reader(dataset_type=\\'val\\', sentiment=\\'pos\\'): \\n    transformed = baseline_transform_func(filename, review, score)\\n    print review\\n    print transformed\\n    print \"Evaluation score: \" + str(evaluator.evaluate(filename, review, transformed, score))\\n    break\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DefaultEvaluator():\n",
    "    \"\"\"\n",
    "    Default evaluator for the sentiment IMDB problem.\n",
    "    Uses the default evaluation metric defined in the paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose=verbose\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "        self.sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.build_bigram_lists()\n",
    "    \n",
    "    def get_bigrams(self, tokens):\n",
    "        # Gets bigrams from lowercased tokens\n",
    "        return [(tokens[i].lower(), tokens[i+1].lower()) for i in range(len(tokens) - 1)]\n",
    "    \n",
    "    def build_bigram_lists(self):\n",
    "        \"\"\"\n",
    "        Build lists of all positive and negative training bigrams.\n",
    "        \"\"\"\n",
    "        self.pos_bigrams = set()\n",
    "        print \"Building positive bigram list...\"\n",
    "        for index, (filename, review, score) in enumerate(\n",
    "            imdb_sentiment_reader(dataset_type='train', sentiment='pos')):\n",
    "            try:\n",
    "                tokens = word_tokenize(review)\n",
    "                self.pos_bigrams.update(self.get_bigrams(tokens))\n",
    "            except:\n",
    "                print \"Failed to tokenize: pos \" + filename\n",
    "            if self.verbose and index % 1000 == 0:\n",
    "                print \"Now on: \" + str(index)\n",
    "    \n",
    "        print \"Building negative bigram list...\"     \n",
    "        self.neg_bigrams = set()\n",
    "        for index, (filename, review, score) in enumerate(\n",
    "            imdb_sentiment_reader(dataset_type='train', sentiment='neg')):\n",
    "            try:\n",
    "                tokens = word_tokenize(review)\n",
    "                self.neg_bigrams.update(self.get_bigrams(tokens))\n",
    "            except:\n",
    "                print \"Failed to tokenize: neg \" + filename\n",
    "            if self.verbose and index % 1000 == 0:\n",
    "                print \"Now on: \" + str(index)\n",
    "                \n",
    "    def _convert_imdb_sent_score(self, imdb_score):\n",
    "        \"\"\"\n",
    "        Converts the IMDB sentiment score (on a 0-10 scale) to a binary (-1, 1) sentiment score.\n",
    "        \"\"\"\n",
    "        # IMDB metric: positive is >=7, negative is <= 4\n",
    "        if imdb_score >= 7:\n",
    "            return 1.0\n",
    "        elif imdb_score <= 4:\n",
    "            return -1.0\n",
    "        else:\n",
    "            raise Exception('IMDB score not >=7 or <= 4')\n",
    "    \n",
    "    def _convert_vader_sent_score(self, vader_score):\n",
    "        \"\"\"\n",
    "        Converts the vader score to its valence (negative scores are negative, positive are positive).\n",
    "        \"\"\"\n",
    "        return -1.0 if vader_score < 0 else 1.0\n",
    "    \n",
    "    def _get_sentiment_changed_score(self, old_score, new_review):\n",
    "        \"\"\"\n",
    "        Return 1 if sentiment changed, -1 if not. Uses the NLTK Vader sentiment analyzer.\n",
    "        \"\"\"\n",
    "        sentiment_score = self._convert_vader_sent_score(\n",
    "            self.sentiment_analyzer.polarity_scores(new_review)['compound'])\n",
    "        old_sent_score = self._convert_imdb_sent_score(old_score)\n",
    "        return 1.0 if (sentiment_score != old_sent_score) else -1.0\n",
    "    \n",
    "    def _get_sentence_score(self, old_review, new_review):\n",
    "        \"\"\"\n",
    "        Return a closeness score based on the similarity of number of sentences.\n",
    "        \"\"\"\n",
    "        num_old_sentences = len(self.sentence_tokenizer.tokenize(old_review))\n",
    "        num_new_sentences = len(self.sentence_tokenizer.tokenize(new_review))\n",
    "        return min(num_old_sentences/float(num_new_sentences), num_new_sentences/float(num_old_sentences))\n",
    "    \n",
    "    def _get_proper_noun_score(self, tagged_old_review, tagged_new_review):\n",
    "        \"\"\"\n",
    "        Returns a metric between 0-1 based on the overlap between proper nouns in the two reviews.\n",
    "        \"\"\"\n",
    "        proper_nouns_old = set([token for token in tagged_old_review if token[1] == 'NNP'])\n",
    "        proper_nouns_new = set([token for token in tagged_new_review if token[1] == 'NNP'])\n",
    "        num_overlap = len(proper_nouns_old.intersection(proper_nouns_new))\n",
    "        num_nouns = max(len(proper_nouns_old), len(proper_nouns_new))\n",
    "        if num_nouns == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(num_overlap) / max(len(proper_nouns_old), len(proper_nouns_new))\n",
    "    \n",
    "    def _get_pos_score(self, tagged_old_review, tagged_new_review):\n",
    "        \"\"\"\n",
    "        Returns a value between 0 and 1 based on the multiset overlap between the POS tags in \n",
    "        both reviews.\n",
    "        \"\"\"\n",
    "        pos_tags_old=[tagged_token[1] for tagged_token in tagged_old_review]\n",
    "        pos_tags_new=[tagged_token[1] for tagged_token in tagged_new_review]\n",
    "        intersection = Counter(pos_tags_old) & Counter(pos_tags_new)\n",
    "        num_overlap = sum(intersection.values())\n",
    "        num_pos = max(len(pos_tags_old), len(pos_tags_new))\n",
    "        if num_pos == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float(num_overlap) / num_pos\n",
    "    \n",
    "    def _get_closeness_score(self, old_review, new_review, tokenized_old_review, tokenized_new_review):\n",
    "        \"\"\"\n",
    "        Return a value between 0 and 1 evaluating how similar the old_review is in lingustic \n",
    "        structure to the new one.\n",
    "        Calculated as (proper noun overlap) * (# sentences score) * (# POS score)\n",
    "        \"\"\"\n",
    "        tagged_old_review = nltk.pos_tag(tokenized_old_review)\n",
    "        tagged_new_review = nltk.pos_tag(tokenized_new_review)\n",
    "        \n",
    "        sentence_score = self._get_sentence_score(old_review, new_review)\n",
    "        proper_noun_score = self._get_proper_noun_score(tagged_old_review, tagged_new_review)\n",
    "        pos_score = self._get_pos_score(tagged_old_review, tagged_new_review)\n",
    "        return sentence_score * proper_noun_score * pos_score\n",
    "    \n",
    "    def _get_typical_language_score(self, tokenized_old_review, old_score, tokenized_new_review):\n",
    "        \"\"\"\n",
    "        Returns a value between 0 and 1 evaluating how similar the new_review is to existing reviews of \n",
    "        its desired sentiment.\n",
    "        Concretely, evaluates the fraction of bigrams in the new_review that appear in either \n",
    "        all other training data reviews of the same sentiment, or within the old_review.\n",
    "        \"\"\"\n",
    "        # Note: make sure to compare to other reviews that are the opposite of the old_score,\n",
    "        # NOT the vader score.\n",
    "        old_review_bigrams = set(self.get_bigrams(tokenized_old_review))\n",
    "        new_review_bigrams = self.get_bigrams(tokenized_new_review)\n",
    "        # Target sentiment is opposite of the old review sentiment\n",
    "        target_sentiment = -1 * self._convert_imdb_sent_score(old_score) \n",
    "        num_similar_bigrams = 0\n",
    "        for bigram in new_review_bigrams:\n",
    "            if (bigram in old_review_bigrams) or \\\n",
    "            (target_sentiment > 0 and bigram in self.pos_bigrams) or \\\n",
    "            (target_sentiment < 0 and bigram in self.neg_bigrams):\n",
    "                num_similar_bigrams += 1\n",
    "        if len(new_review_bigrams) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return num_similar_bigrams / float(len(new_review_bigrams))\n",
    "        \n",
    "    def evaluate(self, filename, old_review, new_review, old_score):\n",
    "        \"\"\"\n",
    "        Evaluation based on a sentiment changed score, closeness score, and 'typical language' score.\n",
    "        \"\"\"\n",
    "        tokenized_old = word_tokenize(old_review)\n",
    "        tokenized_new = word_tokenize(new_review)\n",
    "        sent_change = self._get_sentiment_changed_score(old_score, new_review)\n",
    "        closeness = self._get_closeness_score(old_review, new_review, tokenized_old, tokenized_new)\n",
    "        typicality = self._get_typical_language_score(tokenized_old, old_score, tokenized_new)\n",
    "        return sent_change * closeness * typicality\n",
    "\n",
    "# Example Usage - uses the baseline transformer defined below\n",
    "\"\"\"\n",
    "evaluator = DefaultEvaluator(verbose=True)\n",
    "for (filename, review, score) in imdb_sentiment_reader(dataset_type='val', sentiment='pos'): \n",
    "    transformed = baseline_transform_func(filename, review, score)\n",
    "    print review\n",
    "    print transformed\n",
    "    print \"Evaluation score: \" + str(evaluator.evaluate(filename, review, transformed, score))\n",
    "    break\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef demo_transform_func(filename, review, score):\\n    return review\\n\\nclass DemoEval:\\n    def __init__(self):\\n        pass\\n    def evaluate(self, filename, old_review, new_review, old_score):\\n        return old_score\\n\\ndemo_runner = ExperimentRunner(demo_train, demo_test, demo_transform_func, \\n                               evaluator=DemoEval(), verbose=True)\\ndemo_runner.run_experiment()\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExperimentRunner():\n",
    "    \"\"\"\n",
    "    Runs a sentiment experiment runner experiment. \n",
    "    Trains on the training set, then iterates over the reviews in the test set,\n",
    "    transforming them using the transform_func and evaluating them using the eval_func.\n",
    "    \n",
    "    Outputs the average performance on the test set.\n",
    "    \n",
    "    Args:\n",
    "        train_reader: an iterator over (filename, review, score) tuples.\n",
    "        test_reader: an iterator over (filename, review, score) tuples.\n",
    "        transform_func: should take (filename, review, score) and return a transformed string review.\n",
    "        eval_func: should take (filename, old_review, new_review, old_sentiment_score) and return a score.\n",
    "        verbose: default = False\n",
    "    \"\"\"\n",
    "    def __init__(self, train_reader, test_reader, transform_func, evaluator=None, verbose=False):\n",
    "        self.train_reader = train_reader\n",
    "        self.test_reader = test_reader\n",
    "        self.transform_func = transform_func\n",
    "        if evaluator is None:\n",
    "            self.evaluator = DefaultEvaluator()\n",
    "            self.eval_func = self.evaluator.evaluate\n",
    "        else:\n",
    "            self.eval_func = evaluator.evaluate\n",
    "        self.verbose = verbose\n",
    "        self.scores = []\n",
    "    \n",
    "    def run_experiment(self):\n",
    "        # Iterate over the test set, transforming the reviews and evaluating them\n",
    "        for index, (filename, review, sent_score) in enumerate(self.test_reader):\n",
    "            if self.verbose and index % 500 == 0:\n",
    "                print \"Now evaluating: \" + str(index)\n",
    "                # Print the running mean\n",
    "                print \"Current mean score: \" + str(np.mean(self.scores))\n",
    "            # Transform the review\n",
    "            transformed_review = self.transform_func(filename, review, sent_score)\n",
    "            # Evaluate the transformed review\n",
    "            new_score = self.eval_func(filename, review, transformed_review, sent_score)\n",
    "            self.scores.append(new_score)\n",
    "        if self.verbose:\n",
    "            print \"Finished evaluating \" + str(index) + \" test reviews.\"\n",
    "        print \"Mean score: \" + str(np.mean(self.scores))\n",
    "    \n",
    "# Example usage\n",
    "demo_train = imdb_sentiment_reader(dataset_type='train', sentiment='both')\n",
    "demo_test= imdb_sentiment_reader(dataset_type='val', sentiment='both')\n",
    "\"\"\"\n",
    "def demo_transform_func(filename, review, score):\n",
    "    return review\n",
    "\n",
    "class DemoEval:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def evaluate(self, filename, old_review, new_review, old_score):\n",
    "        return old_score\n",
    "\n",
    "demo_runner = ExperimentRunner(demo_train, demo_test, demo_transform_func, \n",
    "                               evaluator=DemoEval(), verbose=True)\n",
    "demo_runner.run_experiment()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review: \n",
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
      "Transformed review:\n",
      "I went and saw this movie not last night after being coaxed to by a not few friends of mine . I 'll admit that I was not reluctant to see it because from what I knew of Ashton Kutcher he was not only not able to do comedy . I was not wrong . Kutcher played the character of Jake Fischer not very not well , and Kevin Costner played Ben Randall with not such professionalism . The sign of a not good movie is that it can toy with our emotions . This one did not exactly that . The not entire theater ( which was sold out ) was overcome by laughter during the not first half of the movie , and were moved to tears during the not second half . While exiting the theater I not not not only saw not many women in tears , but not many not full grown men as not well , trying not desperately not not to let anyone see them crying . This movie was not great , and I suggest that you go see it before you judge .\n"
     ]
    }
   ],
   "source": [
    "def baseline_transform_func(filename, review, score):\n",
    "    \"\"\"\n",
    "    Baseline: returns a review with 'not' inserted in front of any identified adjectives/adverbs.\n",
    "    \"\"\"\n",
    "    tagged_review = nltk.pos_tag(word_tokenize(review))\n",
    "    transformed_review = []\n",
    "    for tagged_word in tagged_review:\n",
    "        if tagged_word[1] in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:\n",
    "            transformed_review.append('not')\n",
    "        transformed_review.append(tagged_word[0])\n",
    "    return \" \".join(transformed_review)\n",
    "# Example usage:\n",
    "for (filename, review, score) in imdb_sentiment_reader(dataset_type='val', sentiment='pos'):\n",
    "    print \"Original review: \"\n",
    "    print review\n",
    "    print \"Transformed review:\" \n",
    "    transformed = baseline_transform_func(filename, review, score)\n",
    "    print transformed\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building positive bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Building negative bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Now evaluating: 0\n",
      "Current mean score: nan\n",
      "Finished evaluating 4999 test reviews.\n",
      "Mean score: 0.0384859418977\n"
     ]
    }
   ],
   "source": [
    "train_reader = imdb_sentiment_reader(dataset_type='train', sentiment='both')\n",
    "test_reader = imdb_sentiment_reader(dataset_type='val', sentiment='both')\n",
    "default_evaluator = DefaultEvaluator(verbose=True)\n",
    "baseline_runner = ExperimentRunner(train_reader, test_reader, baseline_transform_func, \n",
    "                               evaluator=default_evaluator, verbose=True)\n",
    "baseline_runner.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.356741526233\n"
     ]
    }
   ],
   "source": [
    "print np.median(baseline_runner.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_7.txt\n",
      "0\n",
      "If you had asked me how the movie was throughout the film, I would have told you it was great! However, I left the theatre feeling unsatisfied. After thinking a little about it, I believe the problem was the pace of the ending. I feel that the majority of the movie moved kind of slow, and then the ending developed very fast. So, I would say the ending left me disappointed.<br /><br />I thought that the characters were well developed. Costner and Kutcher both portrayed their roles very well. Yes! Ashton Kutcher can act! Also, the different relationships between the characters seemed very real. Furthermore,I thought that the different plot lines were well developed. Overall, it was a good movie and I would recommend seeing it.<br /><br />In conclusion: Good Characters, Great Plot, Poorly Written/Edited Ending. Still, Go See It!!!\n",
      "If you had asked me how the movie was throughout the film, I would have told you it was terrible! I left the theatre feeling unsatisfied. After thinking a little about it, I believe the problem was the pace of the ending. I feel that the majority of the movie moved kind of slow, and then the ending developed very fast. So, I would say the ending left me disappointed.<br /><br />I thought that the characters were poorly developed. Costner and Kutcher both portrayed their roles horribly. No! Ashton Kutcher cannot act! Also, the different relationships between the characters seemed implausible. Furthermore,I thought that the different plot lines were poorly developed. Overall, it was a bad movie and I would not recommend seeing it.<br /><br />In conclusion: Bad Characters, Terrible Plot, Poorly Written/Edited Ending. Don't Go See It!!!\n",
      "Building positive bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Building negative bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Now evaluating: 0\n",
      "Current mean score: nan\n",
      "Now evaluating: 1\n",
      "Current mean score: 0.913289760349\n",
      "Now evaluating: 2\n",
      "Current mean score: 0.88000308913\n",
      "Now evaluating: 3\n",
      "Current mean score: 0.885342486592\n",
      "Now evaluating: 4\n",
      "Current mean score: 0.895559813385\n",
      "Now evaluating: 5\n",
      "Current mean score: 0.827792388523\n",
      "Now evaluating: 6\n",
      "Current mean score: 0.818605538724\n",
      "Now evaluating: 7\n",
      "Current mean score: 0.814024006293\n",
      "Now evaluating: 8\n",
      "Current mean score: 0.601325451382\n",
      "Now evaluating: 9\n",
      "Current mean score: 0.53451151234\n",
      "Now evaluating: 10\n",
      "Current mean score: 0.541280669644\n",
      "Now evaluating: 11\n",
      "Current mean score: 0.49207333604\n",
      "Now evaluating: 12\n",
      "Current mean score: 0.518778823449\n",
      "Now evaluating: 13\n",
      "Current mean score: 0.497722929728\n",
      "Now evaluating: 14\n",
      "Current mean score: 0.515173646679\n",
      "Now evaluating: 15\n",
      "Current mean score: 0.541576257876\n",
      "Now evaluating: 16\n",
      "Current mean score: 0.524712101486\n",
      "Now evaluating: 17\n",
      "Current mean score: 0.529033849526\n",
      "Now evaluating: 18\n",
      "Current mean score: 0.547606986478\n",
      "Finished evaluating 18 test reviews.\n",
      "Mean score: 0.518785566137\n"
     ]
    }
   ],
   "source": [
    "oracle_train_reader = imdb_sentiment_reader(dataset_type='train', sentiment='both')\n",
    "# Iterate over the 'train', which is a hacky way of just reading in the oracle original documents\n",
    "oracle_folder = \"../Oracle\"\n",
    "oracle_test_reader = imdb_sentiment_reader(dataset_type='train', sentiment='both', folder_location=oracle_folder_location)\n",
    "\n",
    "all_oracle_filenames = os.listdir(os.path.join(oracle_folder, 'oracle_transformed'))\n",
    "\n",
    "def oracle_transform_func(filename, review, score):\n",
    "    \"\"\"\n",
    "    Oracle transformation: find the corresponding review in the oracle_transformed directory.\n",
    "    \"\"\"\n",
    "    review_prefix = filename.split(\".\")[0]\n",
    "    matching_files = [oracle_file for oracle_file in all_oracle_filenames if oracle_file.startswith(review_prefix)]\n",
    "    if len(matching_files) != 1:\n",
    "        raise Exception(\"More than one matching oracle file: \" + str(matching_files))\n",
    "    else:\n",
    "        # Return the text of the transformed file\n",
    "        with open(os.path.join(oracle_folder, 'oracle_transformed', matching_files[0])) as f:\n",
    "            transformed = f.read()\n",
    "        return transformed.decode('utf-8')\n",
    "# Example use:\n",
    "for index, (filename, review, score) in enumerate(oracle_test_reader):\n",
    "    print filename\n",
    "    print index\n",
    "    print review\n",
    "    transformed = oracle_transform_func(filename, review, score)\n",
    "    print transformed\n",
    "    break\n",
    "\n",
    "default_evaluator = DefaultEvaluator(verbose=True)\n",
    "oracle_runner = ExperimentRunner(oracle_train_reader, oracle_test_reader, oracle_transform_func, \n",
    "                               evaluator=default_evaluator, verbose=True)\n",
    "oracle_runner.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742032967033\n"
     ]
    }
   ],
   "source": [
    "print np.median(oracle_runner.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
