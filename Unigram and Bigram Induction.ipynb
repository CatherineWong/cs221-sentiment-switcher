{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram and Bigram Induction\n",
    "11/13/16 - Implements unigram and bigram induction.\n",
    "Uses the IMDB dataset folder (http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sentiment_utils import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import cPickle as pickle\n",
    "imdb_folder_location = \"../aclImdb\" # Change this to wherever your imbd folder is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Bigram and Unigram Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n"
     ]
    }
   ],
   "source": [
    "## Builds maps over the neighboring contexts. Each token is weighted based on its frequency in that context over the\n",
    "# training set and its score.\n",
    "pos_both_neighboring_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_preceding_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_following_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_both_neighboring_unigrams = defaultdict(lambda : defaultdict(int))\n",
    "for index, (filename, review, score) in enumerate(imdb_sentiment_reader(dataset_type='train', sentiment='pos')):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the preceding bigram if it exists\n",
    "        preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "        # Get the following bigram if it exists\n",
    "        following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "        # Get the preceding and following unigrams if they exist\n",
    "        preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "        following_unigram = tokens[i+1] if i < len(tokens) - 1 else None\n",
    "        \n",
    "        # Insert into the appropriate maps\n",
    "        if preceding_bigram is not None and following_bigram is not None:\n",
    "            pos_both_neighboring_bigrams[(preceding_bigram, following_bigram)][tokens[i]] += score\n",
    "        if preceding_bigram is not None:\n",
    "            pos_preceding_bigrams[preceding_bigram][tokens[i]] += score\n",
    "        if following_bigram is not None:\n",
    "            pos_following_bigrams[following_bigram][tokens[i]] += score\n",
    "        if preceding_unigram is not None and following_unigram is not None:\n",
    "            pos_both_neighboring_unigrams[(preceding_unigram, following_unigram)][tokens[i]] += score\n",
    "    if index % 1000 == 0:\n",
    "        print \"Now on: \" + str(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n"
     ]
    }
   ],
   "source": [
    "## Builds maps over the neighboring contexts. Each token is weighted based on its frequency in that context over the\n",
    "# training set and its score.\n",
    "neg_both_neighboring_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_preceding_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_following_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_both_neighboring_unigrams = defaultdict(lambda : defaultdict(int))\n",
    "for index, (filename, review, score) in enumerate(imdb_sentiment_reader(dataset_type='train', sentiment='neg')):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the preceding bigram if it exists\n",
    "        preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "        # Get the following bigram if it exists\n",
    "        following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "        # Get the preceding and following unigrams if they exist\n",
    "        preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "        following_unigram = tokens[i+1] if i < len(tokens) - 1 else None\n",
    "        \n",
    "        # Insert into the appropriate maps\n",
    "        if preceding_bigram is not None and following_bigram is not None:\n",
    "            neg_both_neighboring_bigrams[(preceding_bigram, following_bigram)][tokens[i]] -= score\n",
    "        if preceding_bigram is not None:\n",
    "            neg_preceding_bigrams[preceding_bigram][tokens[i]] -= score\n",
    "        if following_bigram is not None:\n",
    "            neg_following_bigrams[following_bigram][tokens[i]] -= score\n",
    "        if preceding_unigram is not None and following_unigram is not None:\n",
    "            neg_both_neighboring_unigrams[(preceding_unigram, following_unigram)][tokens[i]] -= score\n",
    "    if index % 1000 == 0:\n",
    "        print \"Now on: \" + str(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the dictionaries\n",
    "both_neighboring_bigrams = {\"pos\": pos_both_neighboring_bigrams, \"neg\": neg_both_neighboring_bigrams}\n",
    "preceding_bigrams = {\"pos\": pos_preceding_bigrams, \"neg\": neg_preceding_bigrams}\n",
    "following_bigrams = {\"pos\": pos_following_bigrams, \"neg\": neg_following_bigrams}\n",
    "both_neighboring_unigrams = {\"pos\": pos_both_neighboring_unigrams, \"neg\": neg_both_neighboring_unigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review: \n",
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
      "Transformed review:\n",
      "I went and saw this movie last night after being coaxed to by a few friends of mine . I 'll admit that I was hoping to see it because from what I knew of Ashton Kutcher he was never reason to do comedy . I was terrible . Kutcher played the character of Jake Fischer . good , and Kevin Costner played Ben Randall with and professionalism . The sign of a good movie is that it can toy with our emotions . This one did . that . The movie theater ( which was sold out ) was overcome by laughter during the first half of the movie , and were moved to tears during the first half . While exiting the theater I can been saw a women in tears , but the of grown men as well , trying to trying to let anyone see them crying . This movie was ok , and I suggest that you go see it before you judge .\n"
     ]
    }
   ],
   "source": [
    "def induction_transform_func(filename, review, score):\n",
    "    \"\"\"\n",
    "    Baseline: returns a review with 'not' inserted in front of any identified adjectives/adverbs.\n",
    "    \"\"\"\n",
    "    def get_best_replacement(words_to_scores, score_type):\n",
    "        \"\"\"\n",
    "        Attempts to find a replacement, but returns None if the replacement is not the correct part of speech\n",
    "        \"\"\"\n",
    "        if score_type == \"pos\":\n",
    "            return sorted(words_to_scores, key=words_to_scores.get, reverse=True)[0]\n",
    "        else:\n",
    "             return sorted(words_to_scores, key=words_to_scores.get)[0]\n",
    "    \n",
    "    score_type = \"pos\" if score < 7 else \"neg\" # We want the opposite review type\n",
    "    upper_tokens = word_tokenize(review)\n",
    "    tagged_review = nltk.pos_tag(upper_tokens)\n",
    "    transformed_review = []\n",
    "    tokens = [token.lower() for token in upper_tokens]\n",
    "    if len(tokens) != len(tagged_review):\n",
    "        # Return the original review\n",
    "        return review\n",
    "    for i, tagged_word in enumerate(tagged_review):\n",
    "        # Attempt to find a replacement\n",
    "        replacement_token = tagged_word[0]\n",
    "        if tagged_word[1] in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:\n",
    "            # Get all the preceding unigrams_bigrams\n",
    "            # Get the preceding bigram if it exists\n",
    "            preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "            # Get the following bigram if it exists\n",
    "            following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "            # Get the preceding and following unigrams if they exist\n",
    "            preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "            following_unigram = tokens[i+1] if i < len(tokens) - 1 else None    \n",
    "            # Try each in turn, checking if we have the right Part of Speech\n",
    "            if (preceding_bigram, following_bigram) in both_neighboring_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(both_neighboring_bigrams[score_type][(preceding_bigram, following_bigram)], score_type)\n",
    "            elif (preceding_unigram, following_unigram) in both_neighboring_unigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(both_neighboring_unigrams[score_type][(preceding_unigram, following_unigram)], score_type)\n",
    "            elif preceding_bigram in preceding_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(preceding_bigrams[score_type][preceding_bigram], score_type)\n",
    "            elif following_bigram in following_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(following_bigrams[score_type][following_bigram], score_type)\n",
    "        transformed_review.append(replacement_token)\n",
    "        # Only reverse if new word has same POS as old word\n",
    "    return \" \".join(transformed_review)\n",
    "# Example usage:\n",
    "for (filename, review, score) in imdb_sentiment_reader(dataset_type='val', sentiment='pos'):\n",
    "    print \"Original review: \"\n",
    "    print review\n",
    "    print \"Transformed review:\" \n",
    "    transformed = induction_transform_func(filename, review, score)\n",
    "    print transformed\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building positive bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Building negative bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Now evaluating: 0\n",
      "Current mean score: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-c9fb373a11eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m baseline_runner = ExperimentRunner(train_reader, test_reader, induction_transform_func, \n\u001b[1;32m      5\u001b[0m                                evaluator=default_evaluator, verbose=True)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbaseline_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Cathy/Desktop/Cathy/2016-2017 Senior Year/CS221/cs221-project/sentiment_utils.pyc\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Current mean score: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;31m# Transform the review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mtransformed_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;31m# Evaluate the transformed review\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mnew_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_review\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-5da215d5f714>\u001b[0m in \u001b[0;36minduction_transform_func\u001b[0;34m(filename, review, score)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscore_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"pos\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"neg\"\u001b[0m \u001b[0;31m# We want the opposite review type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mupper_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtagged_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtransformed_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupper_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cathy/anaconda/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cathy/anaconda/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cathy/anaconda/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mw_td_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_td_c\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cathy/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cathy/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cathy/anaconda/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_long_binget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_long_binget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_long_binget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_reader = imdb_sentiment_reader(dataset_type='train', sentiment='both')\n",
    "test_reader = imdb_sentiment_reader(dataset_type='val', sentiment='both')\n",
    "default_evaluator = DefaultEvaluator(verbose=True)\n",
    "baseline_runner = ExperimentRunner(train_reader, test_reader, induction_transform_func, \n",
    "                               evaluator=default_evaluator, verbose=True)\n",
    "baseline_runner.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
