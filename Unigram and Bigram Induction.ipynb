{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram and Bigram Induction\n",
    "11/13/16 - Implements unigram and bigram induction.\n",
    "Uses the IMDB dataset folder (http://ai.stanford.edu/~amaas/data/sentiment/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cathy/anaconda/lib/python2.7/site-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from sentiment_utils import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import pickle as pickle\n",
    "imdb_folder_location = \"../aclImdb\" # Change this to wherever your imbd folder is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Bigram and Unigram Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n"
     ]
    }
   ],
   "source": [
    "## Builds maps over the neighboring contexts. Each token is weighted based on its frequency in that context over the\n",
    "# training set and its score.\n",
    "pos_both_neighboring_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_preceding_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_following_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "pos_both_neighboring_unigrams = defaultdict(lambda : defaultdict(int))\n",
    "for index, (filename, review, score) in enumerate(imdb_sentiment_reader(dataset_type='train', sentiment='pos')):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the preceding bigram if it exists\n",
    "        preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "        # Get the following bigram if it exists\n",
    "        following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "        # Get the preceding and following unigrams if they exist\n",
    "        preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "        following_unigram = tokens[i+1] if i < len(tokens) - 1 else None\n",
    "        \n",
    "        # Insert into the appropriate maps\n",
    "        if preceding_bigram is not None and following_bigram is not None:\n",
    "            pos_both_neighboring_bigrams[(preceding_bigram, following_bigram)][tokens[i]] += score\n",
    "        if preceding_bigram is not None:\n",
    "            pos_preceding_bigrams[preceding_bigram][tokens[i]] += score\n",
    "        if following_bigram is not None:\n",
    "            pos_following_bigrams[following_bigram][tokens[i]] += score\n",
    "        if preceding_unigram is not None and following_unigram is not None:\n",
    "            pos_both_neighboring_unigrams[(preceding_unigram, following_unigram)][tokens[i]] += score\n",
    "    if index % 1000 == 0:\n",
    "        print \"Now on: \" + str(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n"
     ]
    }
   ],
   "source": [
    "## Builds maps over the neighboring contexts. Each token is weighted based on its frequency in that context over the\n",
    "# training set and its score.\n",
    "neg_both_neighboring_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_preceding_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_following_bigrams = defaultdict(lambda : defaultdict(int))\n",
    "neg_both_neighboring_unigrams = defaultdict(lambda : defaultdict(int))\n",
    "for index, (filename, review, score) in enumerate(imdb_sentiment_reader(dataset_type='train', sentiment='neg')):\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    for i in range(len(tokens)):\n",
    "        # Get the preceding bigram if it exists\n",
    "        preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "        # Get the following bigram if it exists\n",
    "        following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "        # Get the preceding and following unigrams if they exist\n",
    "        preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "        following_unigram = tokens[i+1] if i < len(tokens) - 1 else None\n",
    "        \n",
    "        # Insert into the appropriate maps\n",
    "        if preceding_bigram is not None and following_bigram is not None:\n",
    "            neg_both_neighboring_bigrams[(preceding_bigram, following_bigram)][tokens[i]] -= score\n",
    "        if preceding_bigram is not None:\n",
    "            neg_preceding_bigrams[preceding_bigram][tokens[i]] -= score\n",
    "        if following_bigram is not None:\n",
    "            neg_following_bigrams[following_bigram][tokens[i]] -= score\n",
    "        if preceding_unigram is not None and following_unigram is not None:\n",
    "            neg_both_neighboring_unigrams[(preceding_unigram, following_unigram)][tokens[i]] -= score\n",
    "    if index % 1000 == 0:\n",
    "        print \"Now on: \" + str(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the dictionaries\n",
    "both_neighboring_bigrams = {\"pos\": pos_both_neighboring_bigrams, \"neg\": neg_both_neighboring_bigrams}\n",
    "preceding_bigrams = {\"pos\": pos_preceding_bigrams, \"neg\": neg_preceding_bigrams}\n",
    "following_bigrams = {\"pos\": pos_following_bigrams, \"neg\": neg_following_bigrams}\n",
    "both_neighboring_unigrams = {\"pos\": pos_both_neighboring_unigrams, \"neg\": neg_both_neighboring_unigrams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original review: \n",
      "I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
      "Transformed review:\n",
      "I went and saw this movie last night after being coaxed to by a few friends of mine . I 'll admit that I was hoping to see it because from what I knew of Ashton Kutcher he was never reason to do comedy . I was terrible . Kutcher played the character of Jake Fischer . good , and Kevin Costner played Ben Randall with and professionalism . The sign of a good movie is that it can toy with our emotions . This one did . that . The movie theater ( which was sold out ) was overcome by laughter during the first half of the movie , and were moved to tears during the first half . While exiting the theater I can been saw a women in tears , but the of grown men as well , trying to trying to let anyone see them crying . This movie was ok , and I suggest that you go see it before you judge .\n"
     ]
    }
   ],
   "source": [
    "def induction_transform_func(filename, review, score):\n",
    "    \"\"\"\n",
    "    Baseline: returns a review with 'not' inserted in front of any identified adjectives/adverbs.\n",
    "    \"\"\"\n",
    "    def get_best_replacement(words_to_scores, score_type):\n",
    "        \"\"\"\n",
    "        Attempts to find a replacement, but returns None if the replacement is not the correct part of speech\n",
    "        \"\"\"\n",
    "        if score_type == \"pos\":\n",
    "            return sorted(words_to_scores, key=words_to_scores.get, reverse=True)[0]\n",
    "        else:\n",
    "             return sorted(words_to_scores, key=words_to_scores.get)[0]\n",
    "    \n",
    "    score_type = \"pos\" if score < 7 else \"neg\" # We want the opposite review type\n",
    "    upper_tokens = word_tokenize(review)\n",
    "    tagged_review = nltk.pos_tag(upper_tokens)\n",
    "    transformed_review = []\n",
    "    tokens = [token.lower() for token in upper_tokens]\n",
    "    if len(tokens) != len(tagged_review):\n",
    "        # Return the original review\n",
    "        return review\n",
    "    for i, tagged_word in enumerate(tagged_review):\n",
    "        # Attempt to find a replacement\n",
    "        replacement_token = tagged_word[0]\n",
    "        if tagged_word[1] in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']:\n",
    "            # Get all the preceding unigrams_bigrams\n",
    "            # Get the preceding bigram if it exists\n",
    "            preceding_bigram = (tokens[i-2], tokens[i-1]) if i >= 2 else None\n",
    "            # Get the following bigram if it exists\n",
    "            following_bigram = (tokens[i+1], tokens[i+2]) if i < len(tokens) - 2 else None\n",
    "            # Get the preceding and following unigrams if they exist\n",
    "            preceding_unigram = tokens[i-1] if i >= 1 else None\n",
    "            following_unigram = tokens[i+1] if i < len(tokens) - 1 else None    \n",
    "            # Try each in turn, checking if we have the right Part of Speech\n",
    "            if (preceding_bigram, following_bigram) in both_neighboring_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(both_neighboring_bigrams[score_type][(preceding_bigram, following_bigram)], score_type)\n",
    "            elif (preceding_unigram, following_unigram) in both_neighboring_unigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(both_neighboring_unigrams[score_type][(preceding_unigram, following_unigram)], score_type)\n",
    "            elif preceding_bigram in preceding_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(preceding_bigrams[score_type][preceding_bigram], score_type)\n",
    "            elif following_bigram in following_bigrams[score_type]:\n",
    "                replacement_token = get_best_replacement(following_bigrams[score_type][following_bigram], score_type)\n",
    "        transformed_review.append(replacement_token)\n",
    "        # Only reverse if new word has same POS as old word\n",
    "    return \" \".join(transformed_review)\n",
    "# Example usage:\n",
    "for (filename, review, score) in imdb_sentiment_reader(dataset_type='val', sentiment='pos'):\n",
    "    print \"Original review: \"\n",
    "    print review\n",
    "    print \"Transformed review:\" \n",
    "    transformed = induction_transform_func(filename, review, score)\n",
    "    print transformed\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building positive bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Building negative bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Now evaluating: 0\n",
      "Current mean score: nan\n",
      "Now evaluating: 500\n",
      "Current mean score: 0.252545553758\n",
      "Now evaluating: 1000\n",
      "Current mean score: 0.251717427313\n",
      "Now evaluating: 1500\n",
      "Current mean score: 0.239297045012\n",
      "Now evaluating: 2000\n",
      "Current mean score: 0.247353615637\n",
      "Now evaluating: 2500\n",
      "Current mean score: 0.24774795475\n",
      "Now evaluating: 3000\n",
      "Current mean score: 0.289172802129\n",
      "Now evaluating: 3500\n",
      "Current mean score: 0.323627150844\n",
      "Now evaluating: 4000\n",
      "Current mean score: 0.345581136392\n",
      "Now evaluating: 4500\n",
      "Current mean score: 0.36199012089\n",
      "Finished evaluating 4999 test reviews.\n",
      "Mean score: 0.376145820935\n",
      "Median score: 0.329041944546\n",
      "Median score: 0.300328732531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cathy/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:59: RuntimeWarning: Mean of empty slice.\n",
      "  warnings.warn(\"Mean of empty slice.\", RuntimeWarning)\n",
      "/Users/Cathy/anaconda/lib/python2.7/site-packages/numpy/core/_methods.py:70: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "train_reader = imdb_sentiment_reader(dataset_type='train', sentiment='both')\n",
    "test_reader = imdb_sentiment_reader(dataset_type='val', sentiment='both')\n",
    "default_evaluator = DefaultEvaluator(verbose=True)\n",
    "baseline_runner = ExperimentRunner(train_reader, test_reader, induction_transform_func, \n",
    "                               evaluator=default_evaluator, verbose=True)\n",
    "baseline_runner.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review = \"If you had asked me how the movie was throughout the film, I would have told you it was great! However, I left the theatre feeling unsatisfied. After thinking a little about it, I believe the problem was the pace of the ending. I feel that the majority of the movie moved kind of slow, and then the ending developed very fast. So, I would say the ending left me disappointed.<br /><br />I thought that the characters were well developed. Costner and Kutcher both portrayed their roles very well. Yes! Ashton Kutcher can act! Also, the different relationships between the characters seemed very real. Furthermore,I thought that the different plot lines were well developed. Overall, it was a good movie and I would recommend seeing it.<br /><br />In conclusion: Good Characters, Great Plot, Poorly Written/Edited Ending. Still, Go See It!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = '10_7.txt'\n",
    "score = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building positive bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "Building negative bigram list...\n",
      "Now on: 0\n",
      "Now on: 1000\n",
      "Now on: 2000\n",
      "Now on: 3000\n",
      "Now on: 4000\n",
      "Now on: 5000\n",
      "Now on: 6000\n",
      "Now on: 7000\n",
      "Now on: 8000\n",
      "Now on: 9000\n",
      "Now on: 10000\n",
      "Now on: 11000\n",
      "Now on: 12000\n",
      "If you had asked me how the movie was throughout the film , I would have told you it was terrible ! ) , I left the theatre feeling unsatisfied . After thinking a movie about it , I believe the problem was the pace of the ending . I feel that the majority of the movie moved kind of slow , and then the ending is too good . no , I would say the ending left me disappointed. < br / > < br / > I thought that the characters were never developed . Costner and Kutcher both portrayed their roles are good . Yes ! Ashton Kutcher can act ! ) , the sexual relationships between the characters seemed to good . Furthermore , I thought that the main plot lines were not developed . however , it was a home movie and I would recommend seeing it. < br / > < br / > In conclusion : repellent Characters , Great Plot , Poorly Written/Edited Ending . however , Go See It ! ! !\n",
      "Evaluation score: 0.236088627049\n"
     ]
    }
   ],
   "source": [
    "transformed = induction_transform_func(filename, review, score)\n",
    "paper_evaluator = DefaultEvaluator(verbose=True) # Initialize an evaluator for the paper\n",
    "print transformed\n",
    "print \"Evaluation score: \" + str(paper_evaluator.evaluate(filename, review, transformed, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
